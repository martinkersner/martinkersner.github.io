- title: Temporal Convolution for Real-time Keyword Spotting on Mobile Devices
  date: 2019-04-01 00:0:00 +0000
  authors: Seungwoo Choi, Seokjun Seo, Beomjun Shin, Hyeongmin Byun, <b>Martin Kersner</b>, Beomsu Kim, Dongyoung Kim, Sungjoo Ha
  arxiv: https://arxiv.org/abs/1904.03814
  github: https://github.com/hyperconnect/TC-ResNet
  abstract: Keyword spotting (KWS) plays a critical role in enabling speech-based user interactions on smart devices. Recent developments in the field of deep learning have led to wide adoption of convolutional neural networks (CNNs) in KWS systems due to their exceptional accuracy and robustness. The main challenge faced by KWS systems is the trade-off between high accuracy and low latency. Unfortunately, there has been little quantitative analysis of the actual latency of KWS models on mobile devices. This is especially concerning since conventional convolution-based KWS approaches are known to require a large number of operations to attain an adequate level of performance. In this paper, we propose a temporal convolution for real-time KWS on mobile devices. Unlike most of the 2D convolution-based KWS approaches that require a deep architecture to fully capture both low- and high-frequency domains, we exploit temporal convolutions with a compact ResNet architecture. In Google Speech Command Dataset, we achieve more than <b>385x</b> speedup on Google Pixel 1 and surpass the accuracy compared to the state-of-the-art model. In addition, we release the implementation of the proposed and the baseline models including an end-to-end pipeline for training models and evaluating them on mobile devices.

- title: Towards Real-Time Automatic Portrait Matting on Mobile Devices
  date: 2018-10-01 00:0:00 +0000
  authors: Seokjun Seo, Seungwoo Choi, <b>Martin Kersner</b>, Beomjun Shin, Hyungsuk Yoon, Hyeongmin Byun, Sungjoo Ha
  arxiv: https://arxiv.org/abs/1904.03816
  github: https://github.com/hyperconnect/MMNet
  abstract: We tackle the problem of automatic portrait matting on mobile devices. The proposed model is aimed at attaining real-time inference on mobile devices with minimal degradation of model performance. Our model MMNet, based on multi-branch dilated convolution with linear bottleneck blocks, outperforms the state-of-the-art model and is orders of magnitude faster. The model can be accelerated four times to attain 30 FPS on Xiaomi Mi 5 device with moderate increase in the gradient error. Under the same conditions, our model has an order of magnitude less number of parameters and is faster than Mobile DeepLabv3 while maintaining comparable performance.
